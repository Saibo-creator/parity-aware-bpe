#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Authors: Rico Sennrich, Negar Foroutan

""" Parity-aware BPE learns a tokenization that ensures parity in token lengths across languages on a multi-parallel development set.
Unlike standard BPE, which optimizes merges based on a single corpus, this approach explicitly considers cross-lingual fairness during the tokenization process.
"""

from __future__ import unicode_literals

import os
import sys
import inspect
import codecs
import re
import copy
import argparse
import warnings
import tempfile
import functools
import operator
import numpy
import logging
import numpy as np

from multiprocessing import Pool, cpu_count
from collections import defaultdict, Counter, deque
from contextlib import contextmanager

from tokenizers.pre_tokenizers import Whitespace, ByteLevel
from tokenizers import pre_tokenizers
from tokenizers import Tokenizer, models, decoders, normalizers
from tokenizers.processors import TemplateProcessing
SPECIAL_TOKENS = ["<s>", "<pad>", "</s>", "<unk>"]

# Create a logger
logger = logging.getLogger(__name__)
pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), ByteLevel(use_regex=False, add_prefix_space=True)])
decoder = decoders.ByteLevel()


try:
    from tqdm import tqdm
    tqdm.monitor_interval = 0
except ImportError:
    def tqdm(iterator, *args, **kwargs):
        return iterator


def set_logger(verbose=True):
    if verbose:
        level = logging.INFO
    else: 
        level = logging.WARN
    logger.setLevel(level)

    # Create console handler and set level to info
    ch = logging.StreamHandler()
    ch.setLevel(level)

    # Create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)

    # Add handler to logger
    logger.addHandler(ch)


def create_parser(subparsers=None):

    if subparsers:
        parser = subparsers.add_parser('parity-aware-learn-bpe',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description="learn Parity-aware BPE-based word segmentation")
    else:
        parser = argparse.ArgumentParser(
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description="learn Parity-aware BPE-based word segmentation")

    parser.add_argument(
        '--variant', type=str, default='base',
        help="Partiy-aware BPE variant, either 'base' or 'window'. Default: %(default)s")
    parser.add_argument(
        '--input', '-i', type=argparse.FileType('r'), default=[sys.stdin], nargs='*',
        metavar='PATHS',
        help="Input texts (default: standard input).")
    parser.add_argument(
        '--dev', '-d', type=argparse.FileType('r'), nargs='*',
        metavar='PATHS',
        help="Development texts (are used for parity computation).")
    parser.add_argument(
        '--ratio', '-r', type=float, nargs='*',
        help="Desired ratio of compression (comparing to pre-tokenized length) per input language. Can be used for parity computation in lieu of development set.")
    parser.add_argument(
        '--output', '-o', type=argparse.FileType('w'), default=sys.stdout,
        metavar='PATH',
        help="Output file for BPE codes (default: standard output)")
    parser.add_argument(
        '--json-output', '-j', type=str, default=None,
        metavar='PATH',
        help="Output file for Hugging Face tokenizer.json (default: None)")
    parser.add_argument(
        '--symbols', '-s', type=int, default=10000,
        help="Create this many new symbols (each representing a character n-gram) (default: %(default)s)")
    parser.add_argument(
        '--global-merges', '-g', type=int, default=0,
        help="For first INT merge operations, do merge based on global statistics instead of parity-driven language-specifc ones (default: %(default)s)")
    parser.add_argument(
        '--min-frequency', type=int, default=2, metavar='FREQ',
        help='Stop if no symbol pair has frequency >= FREQ (default: %(default)s)')
    parser.add_argument(
        '--preload', type=argparse.FileType('r'), default=None,
        metavar='PATH',
        help="Preload merges from BPE file (default: None). Can be used to continue learning with different settings (e.g. without whitespace pre-tokenization for SuperBPE).")
    parser.add_argument(
        '--pretokenize', type=str, default=['whitespace', 'bytelevel'], nargs='*',
        choices=['whitespace', 'bytelevel'],
        help="Huggingface pre-tokenizer(s) to apply. (default: %(default)s)")
    parser.add_argument('--dict-input', action="store_true",
        help="If set, input file is interpreted as a dictionary where each line contains a word-count pair")
    parser.add_argument(
        '--total-symbols', '-t', action="store_true",
        help="subtract number of characters from the symbols to be generated (so that '--symbols' becomes an estimate for the total number of symbols needed to encode text).")
    parser.add_argument(
        '--window-size', '-w', type=int, default=100,
        help="Size of the context window for the moving-window balancingvariant of parity-aware BPE (default: %(default)s)")
    parser.add_argument(
        '--alpha', type=int, default=2,
        help="Ratio of the context window for the moving-window balancing variant of parity-aware BPE (default: %(default)s)")
    parser.add_argument(
        '--num-workers', type=int, default=1,
        help="Number of processors to process texts, only supported in Python3. If -1, set `multiprocessing.cpu_count()`. (default: %(default)s)")
    parser.add_argument(
        '--verbose', '-v', action="store_true",
        help="verbose mode.")

    return parser

def get_vocabulary(fobj, is_dict=False, num_workers=1):
    """ Reads text and return dictionary that encodes vocabulary.
    Args:
        fobj (file-like object): The input file object to read from.
        is_dict (bool): If True, the input is treated as a dictionary file.
        num_workers (int): The number of worker processes to use for parallel processing.
    Returns:
        Counter: A Counter object mapping tuple(word) to their frequencies.
    """
    vocab = Counter()

    strip_chars = '\r\n '
    split_char = ' '

    if is_dict:
        for i, line in enumerate(fobj):
            try:
                word, count = line.strip(strip_chars).split(split_char)
            except:
                print('Failed reading vocabulary file at line {0}: {1}'.format(i, line))
                sys.exit(1)
            vocab[tuple(word)] += int(count)
    elif num_workers == 1 or fobj.name == '<stdin>':
        if num_workers > 1:
            warnings.warn("In parallel mode, the input cannot be STDIN. Using 1 processor instead.")
        for i, line in enumerate(fobj):
            # spliting the line using huggingface bpe-pre_tokenizer
            split_line = [item[0] for item in pre_tokenizer.pre_tokenize_str(line)]
            for word in split_line:
                if word:
                    vocab[tuple(word)] += 1
            
    elif num_workers > 1:

        with open_file(fobj.name, 'rb') as f: 
            size = os.fstat(f.fileno()).st_size
            chunk_size = int(size / num_workers)
            offsets = [0 for _ in range(num_workers + 1)]
            
            # Set the final offset to the end of the file
            offsets[num_workers] = size 

            for i in range(1, num_workers):
                f.seek(chunk_size * i)
                pos = f.tell()
                
                # Read to the next line break
                f.readline()
                
                offsets[i] = f.tell()
                assert 0 <= offsets[i] < 1e20, "Bad new line separator"

        pool = Pool(processes=num_workers)
        results = []
        for i in range(num_workers):
            # Pass the file *name* and offsets to the worker
            res = pool.apply_async(_get_vocabulary, (fobj.name, offsets[i], offsets[i + 1]))
            results.append(res)
        
        pool.close()
        pool.join()

        # Collect results and sum the Counters
        for res in results:
            worker_vocab = res.get()
            vocab.update(worker_vocab)
            
    else:
        raise ValueError('`num_workers` is expected to be a positive number, but got {}.'.format(num_workers))
    return vocab

def _get_vocabulary(infile, begin, end):
    vocab = Counter()
    
    # Open in binary mode to use the byte offsets
    with open_file(infile, 'rb') as f:
        f.seek(begin)
        
        # Read the first line (as bytes)
        line_bytes = f.readline() 
        
        while line_bytes:
            pos = f.tell()
            assert 0 <= pos < 1e20, "Bad new line separator, e.g. '\\r'"
            # Stop if we've read past the end offset
            if end > 0 and pos > end: 
                break
                
            # Decode the bytes into a string
            try:
                line_str = line_bytes.decode('utf-8', errors='strict')
            except UnicodeDecodeError as e:
                logger.warning(f"Skipping line with invalid UTF-8 at position {f.tell()}: {e}")
                line_bytes = f.readline() 
                continue 
            
            split_line = [item[0] for item in pre_tokenizer.pre_tokenize_str(line_str)]
            for word_str in split_line:
                if word_str:
                    vocab[tuple(word_str)] += 1
                        
            # Read the next line (as bytes)
            line_bytes = f.readline() 
            
    # Return the Counter directly
    return vocab


def pre_merge(vocab, bpe_codes):
    """Apply list of BPE merge operations to each item in vocab.
    
    Args:
        vocab (Counter): mapping from tuple(chars) -> count (tuple-keyed).
        bpe_codes (dict): mapping from (str, str) pair -> merge rank.
    
    Returns:
        Counter: new vocab with merges applied, still tuple-keyed.
    """

    new_vocab = Counter()

    for orig in vocab:

        if len(orig) == 1:
            new_vocab[orig] = vocab[orig]
            continue

        word = list(orig)

        while len(word) > 1:

            # get list of symbol pairs; optionally apply dropout
            pairs = [(bpe_codes[pair], i, pair) for (i, pair) in enumerate(zip(word, word[1:])) if pair in bpe_codes]

            if not pairs:
                break

            # get first merge operation in list of BPE codes
            bigram = min(pairs)[2]

            # find start position of all pairs that we want to merge
            positions = [i for (rank, i, pair) in pairs if pair == bigram]

            i = 0
            new_word = []
            bigram_str = ''.join(bigram)
            for j in positions:
                # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)
                if j < i:
                    continue
                new_word.extend(word[i:j]) # all symbols before merged pair
                new_word.append(bigram_str) # merged pair
                i = j + 2 # continue after merged pair
            new_word.extend(word[i:]) # add all symbols until end of word
            word = new_word

        new_vocab[tuple(word)] = vocab[orig]

    return new_vocab


def _process_vocab_chunk(chunk_data):
    """Process a chunk of vocab and return local stats and indices."""
    vocab_chunk, start_idx, array_len = chunk_data

    local_stats = {}
    local_indices = {}

    for local_i, (word, freq) in enumerate(vocab_chunk):
        global_i = start_idx + local_i
        for p in zip(word[0:-1], word[1:]):
            if p not in local_stats:
                local_stats[p] = numpy.zeros(array_len, dtype=int)
                local_indices[p] = {}
            local_stats[p] += freq
            if global_i not in local_indices[p]:
                local_indices[p][global_i] = 0
            local_indices[p][global_i] += 1

    return local_stats, local_indices


def get_pair_statistics(vocab, num_workers=1):
    """ Counts frequency of all symbol pairs, and create index.
    Args:
        vocab (list): A list of tuples, where each tuple contains a word (as a tuple of characters) and its frequency in each language.
        num_workers (int): Number of parallel workers (default: 1 for single-threaded).
    Returns:
        tuple: A tuple containing two dictionaries:
            - stats (defaultdict): A dictionary mapping symbol pairs (tuples) to their frequencies (numpy arrays).
            - indices (defaultdict): A dictionary mapping symbol pairs (tuples) to their indices (defaultdicts).
    """
    if not vocab:
        return defaultdict(lambda: numpy.zeros(1, dtype=int)), defaultdict(lambda: defaultdict(int))

    array_len = len(vocab[0][1])

    # Single-threaded version
    if num_workers <= 1:
        stats = defaultdict(lambda: numpy.zeros(array_len, dtype=int))
        indices = defaultdict(lambda: defaultdict(int))

        for i, (word, freq) in tqdm(enumerate(vocab), total=len(vocab), desc="Computing pair statistics"):
            for p in zip(word[0:-1], word[1:]):
                stats[p] += freq
                indices[p][i] += 1

        return stats, indices

    # Parallel version
    chunk_size = max(1000, len(vocab) // num_workers)
    chunks = []
    for i in range(0, len(vocab), chunk_size):
        chunk = vocab[i:i + chunk_size]
        chunks.append((chunk, i, array_len))

    print(f"Computing pair statistics with {num_workers} workers ({len(chunks)} chunks)...")

    pool = Pool(processes=num_workers)
    results = list(tqdm(pool.imap(_process_vocab_chunk, chunks),
                       total=len(chunks), desc="Computing pair statistics"))
    pool.close()
    pool.join()

    # Merge results
    stats = defaultdict(lambda: numpy.zeros(array_len, dtype=int))
    indices = defaultdict(lambda: defaultdict(int))

    for local_stats, local_indices in results:
        for p, freq in local_stats.items():
            stats[p] += freq
        for p, idx_dict in local_indices.items():
            for idx, count in idx_dict.items():
                indices[p][idx] += count

    return stats, indices


def count_adjacent_pairs_tuple(word):
    """Returns dict[(sym,sym)] -> count for adjacent pairs in word tuple."""
    d = {}
    if len(word) < 2:
        return d
    prev = word[0]
    for cur in word[1:]:
        key = (prev, cur)
        d[key] = d.get(key, 0) + 1
        prev = cur
    return d


def _compute_word_pair_counts_chunk(words):
    """Worker function to compute pair counts for a chunk of words."""
    return [count_adjacent_pairs_tuple(word) for word in words]

def replace_pair(pair, vocab, indices, stats, word_pair_counts):
    """
    Replaces all ('A','B') with 'AB' in vocab entries referenced by indices[pair],
    and applies stats/indices updates inline using per-word pair-count deltas.
    Returns the usual `changes` list for compatibility.
    """
    first, second = pair
    merged = first + second
    changes = []

    def merge_tuple(word, a, b, ab):
        n = len(word)
        if n < 2:
            return word
        out = []
        i = 0
        while i < n:
            if i + 1 < n and word[i] == a and word[i + 1] == b:
                out.append(ab)
                i += 2
            else:
                out.append(word[i])
                i += 1
        return tuple(out)

    # Only iterate over words that actually contain the pair, per indices[pair]
    for j, occ in list(indices[pair].items()):
        if occ < 1:
            continue

        old_word, wfreq = vocab[j]
        new_word = merge_tuple(old_word, first, second, merged)
        if new_word == old_word:
            continue

        # compute new adjacent-pair multiset
        old_pairs = word_pair_counts[j]
        new_pairs = count_adjacent_pairs_tuple(new_word)

        # two-way delta to minimize dict hits
        touched = set(old_pairs.keys()) | set(new_pairs.keys())
        for p in touched:
            old_c = old_pairs.get(p, 0)
            new_c = new_pairs.get(p, 0)
            d = new_c - old_c
            if d == 0:
                continue
            stats[p] += d * wfreq
            indices[p][j] += d

        # commit new state
        vocab[j] = (new_word, wfreq)
        word_pair_counts[j] = new_pairs
        changes.append((j, new_word, old_word, wfreq))

    return changes


def prune_stats(stats, big_stats, threshold, full_sync=False):
    """ Prunes statistics dict for efficiency of max(). 
    The frequency of a symbol pair never increases, so pruning is generally safe
    (until the most frequent pair is less frequent than a pair we previously pruned)
    big_stats keeps full statistics for when we need to access pruned items
    """
    for item,freq in list(stats.items()):
        if full_sync or numpy.all(freq < threshold):
            del stats[item]
            if numpy.any(freq < 0):
                big_stats[item] += freq
            else:
                big_stats[item] = freq


def _merge_tuple(word, a, b, ab):
    """Helper to merge adjacent (a, b) into ab within a word tuple."""
    n = len(word)
    if n < 2:
        return word
    out = []
    i = 0
    while i < n:
        if i + 1 < n and word[i] == a and word[i + 1] == b:
            out.append(ab)
            i += 2
        else:
            out.append(word[i])
            i += 1
    return tuple(out)

def replace_pair_dict(pair, vocab, indices):
    """
    Optimized in-place merge over the dev vocab dict.
     - vocab:   defaultdict[tuple -> np.ndarray]
     - indices: defaultdict[pair -> set[tuple]]
    """
    first, second = pair
    merged = first + second
    length_change = None

    # Iterate over a static list since we modify indices during the loop
    words_to_process = list(indices[pair])

    for old_word in words_to_process:
        
        # Word may have been deleted by a previous merge in this same loop
        if old_word not in vocab:
            continue
            
        freq = vocab[old_word]
        new_word = _merge_tuple(old_word, first, second, merged)

        if new_word == old_word:
            continue
            
        del vocab[old_word]
        vocab[new_word] += freq  

        # Track length change
        if length_change is None:
            length_change = np.zeros(len(freq), dtype=int)
        length_change += (len(old_word) - len(new_word)) * freq

        # Update indices map: remove old_word from all pair indices
        old_pairs = count_adjacent_pairs_tuple(old_word)
        for p in old_pairs:
            if p in indices and old_word in indices[p]:
                indices[p].remove(old_word)
                if not indices[p]:
                    del indices[p]

        # Add new_word to all pair indices
        new_pairs = count_adjacent_pairs_tuple(new_word)
        for p in new_pairs:
            indices[p].add(new_word)

    if length_change is None:
        length_change = 0

    return length_change


@contextmanager
def open_file(filename, mode):
    if mode in ('r', 'w'):
        f = open(filename, mode, encoding="utf-8")
    elif mode in ('rb', 'wb'):
        f = open(filename, mode)
    try:
        yield f
    finally:
        f.close()


def preprocess_input_data(infiles, devfiles, is_dict=False, total_symbols=False, num_global=0, num_workers=1, bpe_file=None):
    """ Reads input files and creates vocabulary data structure.
    Args:
        infiles (list[str]): A list of input file paths.
        devfiles (list[str]): A list of development file paths.
        is_dict (bool): Whether the input files are in dictionary format.
        total_symbols (bool): Whether to count total symbols.
        num_global (int): The number of global symbols.
        num_workers (int): The number of worker threads to use.
        bpe_file (fobj): file containing merge operations to pre-apply before learning.
    Returns:
        tuple: A tuple containing:
            - dev_vocab (defaultdict): A dictionary mapping subwords to their frequencies in the development set.
            - dev_indices (defaultdict): A dictionary mapping pairs to sets of words in dev_vocab.
            - sorted_vocab (list): A sorted list of tuples, where each tuple contains a subword and its frequency in each language.
            - stats (defaultdict): A dictionary mapping symbol pairs (tuples) to their frequencies (numpy arrays).
            - word_pair_counts (list): Per-word adjacent pair count dicts for sorted_vocab.
            - indices (defaultdict): A dictionary mapping symbol pairs (tuples) to their indices (defaultdicts).
            - big_stats (defaultdict): A dictionary containing full statistics for all symbol pairs.
            - threshold (numpy.ndarray): An array of thresholds for pruning statistics.
            - lengths (numpy.ndarray or None): An array where each element corresponds to the sum of frequency*length for all vocab items in the development set.
            - array_length (int): The length of the vocabulary array, which is the number of languages plus one for concatenation.
    """

    if bpe_file is not None:
        # ignore first line containing version information (if it exists)
        line = bpe_file.readline()
        offset = 1
        if not line.startswith('version'):
            bpe_file.seek(0)
            offset = 0
        
        bpe_codes = [tuple(item.strip('\r\n ').split(' ')) for (n, item) in enumerate(bpe_file.read().rstrip('\n').split('\n'))]

        for i, item in enumerate(bpe_codes):
            if len(item) != 2:
                sys.stderr.write('Error: invalid line {0} in BPE codes file: {1}\n'.format(i+offset, ' '.join(item)))
                sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\n')
                sys.exit(1)

        # some hacking to deal with duplicates (only consider first instance)
        bpe_codes = dict([(code, i) for (i, code) in reversed(list(enumerate(bpe_codes)))])
    else:
        bpe_codes = None

    vocabs = []
    joint_keys = set()
    for f in infiles:
        vocab = get_vocabulary(f, is_dict, num_workers)
        if bpe_codes is not None:
            vocab = pre_merge(vocab, bpe_codes)
        vocabs.append(vocab)
        joint_keys = joint_keys.union(vocab.keys())

    dev_vocabs = []
    dev_keys = set()
    if devfiles:
        for f in devfiles:
            vocab = get_vocabulary(f, is_dict, num_workers)
            if bpe_codes is not None:
                vocab = pre_merge(vocab, bpe_codes)
            dev_vocabs.append(vocab)
            dev_keys = dev_keys.union(vocab.keys())

    array_length = len(vocabs)
    if num_global:
        array_length += 1

    # merge vocabularies. Data structure maps from subword to list of frequency in each language, plus one for concatenation
    vocab = defaultdict(lambda: numpy.zeros(array_length, dtype=int))
    for i, v in enumerate(vocabs):
        for key, cnt in v.items():
            vocab[key][i] = cnt

    if num_global:
        for key in joint_keys:
            vocab[key][-1] = sum(vocab[key])

    # merge dev vocabularies. Data structure maps from subword to list of frequency in each language
    dev_vocab = defaultdict(lambda: numpy.zeros(len(dev_vocabs), dtype=int))
    
    if dev_vocabs:
        for i, v in enumerate(dev_vocabs):
            for key, cnt in v.items():
                dev_vocab[key][i] = cnt

    dev_indices = defaultdict(set)
    for word, freq in dev_vocab.items():
        for p in zip(word[0:-1], word[1:]):
            dev_indices[p].add(word)

    sorted_vocab = sorted(vocab.items(), key=lambda x: sum(x[1]), reverse=True)
    stats, indices = get_pair_statistics(sorted_vocab, num_workers=num_workers)

    print("Creating backup of statistics...")
    big_stats = copy.deepcopy(stats)

    print("Computing word-level pair counts...")
    if num_workers <= 1 or len(sorted_vocab) < 10000:
        # Single-threaded version
        word_pair_counts = [count_adjacent_pairs_tuple(word) for (word, _freq) in tqdm(sorted_vocab, desc="Word pair counts")]
    else:
        # Parallel version
        words = [word for (word, _freq) in sorted_vocab]
        chunk_size = max(1000, len(words) // num_workers)
        chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]

        pool = Pool(processes=num_workers)
        results = list(tqdm(
            pool.imap(_compute_word_pair_counts_chunk, chunks),
            total=len(chunks),
            desc=f"Word pair counts ({num_workers} workers)"
        ))
        pool.close()
        pool.join()

        # Flatten results
        word_pair_counts = []
        for chunk_result in results:
            word_pair_counts.extend(chunk_result)

    if total_symbols:
        print("Analyzing character distribution...")
        uniq_char_internal = set()
        uniq_char_final = set()
        for word in tqdm(vocab, desc="Character analysis"):
            for char in word[:-1]:
                uniq_char_internal.add(char)
            uniq_char_final.add(word[-1])
        sys.stderr.write('Number of word-internal characters: {0}\n'.format(len(uniq_char_internal)))
        sys.stderr.write('Number of word-final characters: {0}\n'.format(len(uniq_char_final)))
        sys.stderr.write('Reducing number of merge operations by {0}\n'.format(len(uniq_char_internal) + len(uniq_char_final)))
        # num_symbols -= len(uniq_char_internal) + len(uniq_char_final)

    # threshold is inspired by Zipfian assumption, but should only affect how often we re-sync with full big_stats
    print("Computing pruning thresholds...")
    threshold = numpy.zeros(array_length, dtype=float)
    for l in tqdm(range(array_length), desc="Thresholds"):
        threshold[l] = stats[max(stats, key=lambda x: (stats[x][l], x))][l] / 10

    if dev_vocab:
        print("Computing development set lengths...")
        lengths = functools.reduce(numpy.add, [len(key)*value for key, value in dev_vocab.items()])
    else:
        lengths = None
    return (dev_vocab, dev_indices, sorted_vocab, stats, word_pair_counts, indices, big_stats, threshold, lengths, array_length)


def save_tokenizer_json(path, vocab, merges, unk_token="<unk>", special_tokens=None):
    """
    Saves the learned vocab and merges to a Hugging Face tokenizer.json file.
    
    Args:
        path (str): The file path to save to.
        vocab (dict): The complete vocabulary (str -> int).
        merges (list): The list of merge rules as (str, str) tuples.
        unk_token (str): The unknown token.
        special_tokens (list): List of special token strings.
    """
    if special_tokens is None:
        special_tokens = SPECIAL_TOKENS

    # 1. Create the BPE model from the learned vocab and merges
    bpe_model = models.BPE(
        vocab=vocab,
        merges=merges,
        unk_token=unk_token
    )

    # 2. Create the Tokenizer
    tokenizer = Tokenizer(bpe_model)
    tokenizer.pre_tokenizer = pre_tokenizer 

    # 3. Set the Decoder (must match pre-tokenizer ByteLevel)
    tokenizer.decoder = decoder

    tokenizer.add_special_tokens(special_tokens)
    tokenizer.model.unk_token = unk_token

    # 4. Set post-processor
    tokenizer.post_processor = TemplateProcessing(
        single="<s> $A </s>",
        pair="<s> $A </s> </s> $B </s>",
        special_tokens=[
            ("<s>", tokenizer.token_to_id("<s>")),
            ("</s>", tokenizer.token_to_id("</s>")),
        ]
    )
    # 5. Save the file
    try:
        tokenizer.save(path)
        logger.info(f"Successfully saved Hugging Face tokenizer to {path}")
    except Exception as e:
        logger.error(f"Failed to save tokenizer.json to {path}: {e}")


def learn_bpe(infiles, outfile, devfiles, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False, num_global=0, ratio=None, num_workers=1, bpe_file=None):
    """
    Learn `num_symbols` merge operations using Parity-aware BPE from the provided training and development files
    and write the learned BPE operations to `outfile`.

    Args:
        infiles (list[str]): 
            List of paths to the input text files used for training the BPE model.
        outfile (file-like or str): 
            Path to the file where the learned BPE merge operations will be saved.
        devfiles (list[str]): 
            List of development text files, used for validation during BPE learning.
        num_symbols (int): 
            Number of BPE merge operations to learn.
        min_frequency (int, optional): 
            Minimum frequency threshold for a pair to be considered during merging. Defaults to 2.
        verbose (bool, optional): 
            If True, print detailed logs to stderr during training. Defaults to False.
        is_dict (bool, optional): 
            If True, interpret the input files as dictionaries with frequency counts. Defaults to False.
        total_symbols (bool, optional): 
            If True, subtract number of characters from the symbols to be generated (so that 'num_symbols' becomes an estimate for the total number of symbols needed to encode text). Defaults to False.
        num_global (int, optional): 
            Number of initial merges to perform globally across all corpora before handling them separately. Defaults to 0.
        ratio (list[float]): 
            Desired ratio of compression (comparing to pre-tokenized length) per input language. Can be used for parity computation in lieu of development set.
        num_workers (int, optional): 
            Number of worker processes to use for parallel computations (if supported). Defaults to 1.
        bpe_file (file-like, optional):
            Path to file from which to pre-load BPE merges (to continue learning with different settings, e.g. for SuperBPE).

    Returns:
        tuple: (vocab, merges) for optional HF tokenizer.json export.
    """
    logger.info("Learning parity-aware BPE with the following parameters:"
          "\n  num_symbols: {0}, min_frequency: {1}, verbose: {2}, is_dict: {3}, total_symbols: {4}, num_global: {5}, num_workers: {6}".format(
              num_symbols, min_frequency, verbose, is_dict, total_symbols, num_global, num_workers))
    
    # --- HF Tokenizer Init ---
    initial_alphabet = pre_tokenizers.ByteLevel.alphabet()
    vocab = {token: i for i, token in enumerate(SPECIAL_TOKENS + initial_alphabet)}
    merges = []

    # version numbering allows backward compatibility
    outfile.write('#version: 0.2\n')
    dev_vocab, dev_indices, sorted_vocab, stats, word_pair_counts, indices, big_stats, threshold, lengths, array_length = \
        preprocess_input_data(infiles, devfiles, is_dict, total_symbols, num_global, num_workers, bpe_file)

    if ratio is not None:
        print("Computing initial compression ratios...")
        initial_lengths = functools.reduce(numpy.add, [len(key)*value for key, value in sorted_vocab])
        lengths = numpy.copy(initial_lengths)

    print(f"Starting BPE training loop ({num_symbols} merges)...")
    for i in tqdm(range(num_symbols), desc="parity-aware BPE..."):
        if stats:
            if i < num_global:
                logger.info('lengths {0}: picking best subword based on concatenation'.format(lengths))
                max_index = -1

            else:
                if ratio is not None:
                    # find the language with the least compression, adjusted by desired ratio
                    compression_rates = initial_lengths / lengths
                    adjusted_compression_rates = compression_rates / ratio
                    max_index, max_value = min(enumerate(adjusted_compression_rates), key=operator.itemgetter(1))
                    logger.info('initial lengths {0}\nlengths {1}'.format(initial_lengths, lengths))
                    logger.info('compression rates {0}\nadjusted compression rates {1}: picking best subword in corpus {2}'.format(
                        compression_rates, adjusted_compression_rates, max_index))
                else:
                    max_index, max_value = max(enumerate(lengths), key=operator.itemgetter(1))
                    logger.info('lengths {0}: picking best subword in corpus {1}'.format(lengths, max_index))

            most_frequent = max(stats, key=lambda x: (stats[x][max_index], x))

        # we probably missed the best pair because of pruning; go back to full statistics
        if not stats or (i and stats[most_frequent][max_index] < threshold[max_index]):
            prune_stats(stats, big_stats, threshold, full_sync=True)
            stats = copy.deepcopy(big_stats)
            most_frequent = max(stats, key=lambda x: (stats[x][max_index], x))

            # threshold is inspired by Zipfian assumption, but should only affect how often we re-sync with full big_stats
            for l in range(array_length):
                threshold[l] = stats[max(stats, key=lambda x: (stats[x][l], x))][l] * i/(i+10000.0)

            prune_stats(stats, big_stats, threshold)

        if stats[most_frequent][max_index] < min_frequency:
            sys.stderr.write(f'no pair has frequency >= {min_frequency}. Stopping for language {max_index} with length: {lengths}\n')
            break

        logger.info('pair {0}: {1} {2} -> {1}{2} (frequency {3})'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))

        s1, s2 = most_frequent
        merged_token = s1 + s2
        
        # Add to merges list
        merges.append((s1, s2))
        
        # Add new merged token to vocab
        if merged_token not in vocab:
            vocab[merged_token] = len(vocab)
        
        # Write to BPE codes file
        outfile.write(f"{s1} {s2}\n")
        
        changes = replace_pair(most_frequent, sorted_vocab, indices, stats, word_pair_counts)

        if ratio is not None:
            length_change = functools.reduce(numpy.add, [(len(c[2])-len(c[1]))*c[3] for c in changes])
            lengths -= length_change
        else:
            length_change = replace_pair_dict(most_frequent, dev_vocab, dev_indices)
            lengths -= length_change
        
        if not i % 100:
            prune_stats(stats, big_stats, threshold)

        stats[most_frequent] = numpy.zeros(array_length, dtype=int)
        indices[most_frequent] = defaultdict(int)

    return vocab, merges
    

def select_language_index(lengths, selected_indices, selection_threshold, window_size):
    """ Selects the index of the language with the maximum length from the remaining valid indices.
    The selection is based on a moving window approach, where indices that have been selected too often
    are excluded from further consideration.

    Args:
        lengths (numpy.ndarray): An array of lengths for each language.
        selected_indices (deque): A deque containing the indices of previously selected languages.
        selection_threshold (float): The threshold ratio for selecting an index.
        window_size (int): The size of the moving window.

    Returns:
        int: The index of the selected language.
    """
    final_index = -1
    # Boolean mask to keep track of valid indices
    mask = numpy.ones(len(lengths), dtype=bool)  # Start with all elements valid

    while True:
        # Find the maximum index in the remaining elements
        valid_indices = numpy.where(mask)[0]  # Indices of unmasked elements
        max_index = valid_indices[numpy.argmax(lengths[valid_indices])]  # Max in valid range
        count = selected_indices.count(max_index)
        ratio = count * 1.0 / window_size
        if ratio <= selection_threshold:
            final_index = max_index
            break
        else:
            # Exclude this index from further consideration
            mask[max_index] = False

    return final_index

def learn_bpe_moving_window(infiles, outfile, devfiles, num_symbols, window_size=100, alpha=2, min_frequency=2, verbose=False, is_dict=False, total_symbols=False, num_global=0, ratio=None, num_workers=1, bpe_file=None):
    """
    Learn `num_symbols` merge operations using Parity-aware BPE (moving-window balancing variant) from the provided training and development files
    and write the learned BPE operations to `outfile`.

    Args:
        infiles (list[str]): 
            List of paths to the input text files used for training the BPE model.
        outfile (file-like or str): 
            Path to the file where the learned BPE merge operations will be saved.
        devfiles (list[str]): 
            List of development text files, used for validation during BPE learning.
        num_symbols (int): 
            Number of BPE merge operations to learn.
        window_size (int, optional): 
            Size of the context window for the moving-window balancing variant of parity-aware BPE. Defaults to 100.
        alpha (int, optional): 
            Ratio of the context window for the moving-window balancing variant of parity-aware BPE. Defaults to 2.
        min_frequency (int, optional): 
            Minimum frequency threshold for a pair to be considered during merging. Defaults to 2.
        verbose (bool, optional): 
            If True, print detailed logs to stderr during training. Defaults to False.
        is_dict (bool, optional): 
            If True, interpret the input files as dictionaries with frequency counts. Defaults to False.
        total_symbols (bool, optional): 
            If True, subtract number of characters from the symbols to be generated (so that 'num_symbols' becomes an estimate for the total number of symbols needed to encode text). Defaults to False.
        num_global (int, optional): 
            Number of initial merges to perform globally across all corpora before handling them separately. Defaults to 0.
        ratio (list[float]): 
            Desired ratio of compression (comparing to pre-tokenized length) per input language. Can be used for parity computation in lieu of development set.
        num_workers (int, optional): 
            Number of worker processes to use for parallel computations (if supported). Defaults to 1.
        bpe_file (file-like, optional):
            Path to file from which to pre-load BPE merges (to continue learning with different settings, e.g. for SuperBPE).

    Returns:
        tuple: (vocab, merges) for optional HF tokenizer.json export.
    """
    logger.info("Using Parity-aware BPE (moving-window variant) with window size {0} and alpha {1}".format(window_size, alpha))
    logger.info("Learning parity-aware BPE with the following parameters:"
          "\n  num_symbols: {0}, min_frequency: {1}, verbose: {2}, is_dict: {3}, total_symbols: {4}, num_global: {5}, num_workers: {6}".format(
              num_symbols, min_frequency, verbose, is_dict, total_symbols, num_global, num_workers))
    
    # --- HF Tokenizer Init ---
    initial_alphabet = pre_tokenizers.ByteLevel.alphabet()
    vocab = {token: i for i, token in enumerate(SPECIAL_TOKENS + initial_alphabet)}
    merges = []

    # if continuing learning on top of existing BPE file, contents of BPE file are included in output
    if bpe_file:
        outfile.write(bpe_file.read())
        bpe_file.seek(0)
    else:
        # version numbering allows backward compatibility
        outfile.write('#version: 0.2\n')

    dev_vocab, dev_indices, sorted_vocab, stats, word_pair_counts, indices, big_stats, threshold, lengths, array_length = \
        preprocess_input_data(infiles, devfiles, is_dict, total_symbols, num_global, num_workers, bpe_file)

    if ratio is not None:
        initial_lengths = functools.reduce(numpy.add, [len(key)*value for key, value in sorted_vocab])
        lengths = numpy.copy(initial_lengths)

    selection_threshold = alpha * 1.0 / len(threshold)
    selected_indices = deque(maxlen=window_size)

    for i in tqdm(range(num_symbols), desc="Parity-aware BPE (moving-window variant)... \n"):
        if stats:
            if i < num_global:
                logger.info('lengths {0}: picking best subword based on concatenation'.format(lengths))
                max_index = -1

            else:
                if ratio is not None:
                    # find the language with the least compression, adjusted by desired ratio
                    compression_rates = initial_lengths / lengths
                    adjusted_compression_rates = compression_rates / ratio
                    max_index = select_language_index(-adjusted_compression_rates, selected_indices, selection_threshold, window_size)
                    selected_indices.append(max_index)
                    logger.info('initial lengths {0}\nlengths {1}'.format(initial_lengths, lengths))
                    logger.info('compression rates {0}\nadjusted compression rates {1}: picking best subword in corpus {2}'.format(
                        compression_rates, adjusted_compression_rates, max_index))
                else:
                    max_index = select_language_index(lengths, selected_indices, selection_threshold, window_size)
                    selected_indices.append(max_index)
                    logger.info('lengths {0}: picking best subword in corpus {1}'.format(lengths, max_index))

            most_frequent = max(stats, key=lambda x: (stats[x][max_index], x))

        # we probably missed the best pair because of pruning; go back to full statistics
        if not stats or (i and stats[most_frequent][max_index] < threshold[max_index]):
            prune_stats(stats, big_stats, threshold, full_sync=True)
            stats = copy.deepcopy(big_stats)
            most_frequent = max(stats, key=lambda x: (stats[x][max_index], x))

            # threshold is inspired by Zipfian assumption, but should only affect how often we re-sync with full big_stats
            for l in range(array_length):
                threshold[l] = stats[max(stats, key=lambda x: (stats[x][l], x))][l] * i/(i+10000.0)

            prune_stats(stats, big_stats, threshold)

        if stats[most_frequent][max_index] < min_frequency:
            sys.stderr.write(f'no pair has frequency >= {min_frequency}. Stopping for language {max_index} with length: {lengths}\n')
            break

        logger.info('pair {0}: {1} {2} -> {1}{2} (frequency {3})'.format(i, most_frequent[0], most_frequent[1], stats[most_frequent]))

        s1, s2 = most_frequent
        merged_token = s1 + s2
        
        # Add to merges list
        merges.append((s1, s2))
        
        # Add new merged token to vocab
        if merged_token not in vocab:
            vocab[merged_token] = len(vocab)
        
        # Write to BPE codes file
        outfile.write(f"{s1} {s2}\n")

        changes = replace_pair(most_frequent, sorted_vocab, indices, stats, word_pair_counts)

        if ratio is not None:
            length_change = functools.reduce(numpy.add, [(len(c[2])-len(c[1]))*c[3] for c in changes])
            lengths -= length_change
        else:
            length_change = replace_pair_dict(most_frequent, dev_vocab, dev_indices)
            lengths -= length_change
        
        if not i % 100:
            prune_stats(stats, big_stats, threshold)

        stats[most_frequent] = numpy.zeros(array_length, dtype=int)
        indices[most_frequent] = defaultdict(int)
    
    return vocab, merges

if __name__ == '__main__':

    currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
    newdir = os.path.join(currentdir, 'subword_nmt')
    if os.path.isdir(newdir):
        warnings.warn(
            "this script's location has moved to {0}. This symbolic link will be removed in a future version. Please point to the new location, or install the package and use the command 'subword-nmt'".format(newdir),
            DeprecationWarning
        )

    parser = create_parser()
    args = parser.parse_args()

    sys.stderr = codecs.getwriter('UTF-8')(sys.stderr.buffer)
    sys.stdout = codecs.getwriter('UTF-8')(sys.stdout.buffer)
    sys.stdin = codecs.getreader('UTF-8')(sys.stdin.buffer)

    set_logger(args.verbose)

    if args.num_workers <= 0:
        args.num_workers = cpu_count() - 1
    
    if sys.version_info < (3, 0):
        print("Python 2 is deprecated. Use Python 3")
        sys.exit(1)

    # if we do parity-oriented merging, number of development sets needs to match number of input corpora
    if args.dev:
        assert(len(args.input) == len(args.dev))

    if args.ratio:
        assert(args.dev is None)
        assert(len(args.input) == len(args.ratio))
        args.ratio = numpy.array(args.ratio)
        # normalize ratios by first value given
        args.ratio = args.ratio / args.ratio[0]

    if args.dev is None and args.ratio is None:
        print("script requires either dev sets or ratios")
        sys.exit(1)

    # read/write files as UTF-8
    for i,f in enumerate(args.input):
        if f.name != '<stdin>':
            args.input[i] = codecs.open(f.name, encoding='utf-8')
    if args.dev:
        for i,f in enumerate(args.dev):
            args.dev[i] = codecs.open(f.name, encoding='utf-8')
    if args.output.name != '<stdout>':
        args.output = codecs.open(args.output.name, 'w', encoding='utf-8')
    if args.preload is None:
        bpe_file = None
    else:
        bpe_file = codecs.open(args.preload.name, encoding='utf-8')

    # Configure pre-tokenizer from CLI args
    pretokenizer_list = []
    for pt in args.pretokenize:
        if pt == 'whitespace':
            pretokenizer_list.append(Whitespace())
        elif pt == 'bytelevel':
            pretokenizer_list.append(ByteLevel(use_regex=False))
        else:
            raise ValueError("pretokenizer {0} is not implemented".format(pt))

    pre_tokenizer = pre_tokenizers.Sequence(pretokenizer_list)

    if args.variant == 'base':
        vocab, merges = learn_bpe(args.input, args.output, args.dev, args.symbols, args.min_frequency, args.verbose, num_global=args.global_merges, is_dict=args.dict_input, total_symbols=args.total_symbols, ratio=args.ratio, num_workers=args.num_workers, bpe_file=bpe_file)
    elif args.variant == 'window':
        vocab, merges = learn_bpe_moving_window(args.input, args.output, args.dev, args.symbols, args.window_size, args.alpha, args.min_frequency, args.verbose, num_global=args.global_merges, is_dict=args.dict_input, total_symbols=args.total_symbols, ratio=args.ratio, num_workers=args.num_workers, bpe_file=bpe_file)
    else:
        raise ValueError("Unknown BPE variant: {0}. Use 'base' or 'window'.".format(args.variant))

    if args.json_output and vocab and merges:
        save_tokenizer_json(args.json_output, vocab, merges)

    # close files
    for f in args.input:
        if f.name != '<stdin>':
            f.close()
    if args.output.name != '<stdout>':
        args.output.close()